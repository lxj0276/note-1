* flume
** flume 架构
Source，ChannelProcessor，Channel，Sink
#+BEGIN_SRC java
  Source  {
      ChannelProcessor  {
               Channel  ch1
               Channel  ch2
               …
      }
  } 
  Sink  {
     Channel  ch; 
  } 
  SinkGroup {
     Channel ch；
     Sink s1；
     Sink s2；
     …
  }
#+END_SRC

** Source 组件
Source 是数据源的总称，我们往往设定好源后，数据将源源不断的被抓取或者被推送。
常见的数据源有：ExecSource，KafkaSource，HttpSource，NetcatSource，JmsSource，AvroSource 等等。

Source 提供了两种机制：PollableSource（轮询拉取）和 EventDrivenSource（事件驱动）
** Channel 组件
Channel 用于连接 Source 和 Sink，Source 将日志信息发送到 Channel，Sink 从 Channel 消费日志信息；Channel 是中转日志信息的一个临时存储，保存有 Source 组件传递过来的日志信息。
** Sink 组件
Sink 负责取出 Channel 中的消息数据，进行相应的存储文件系统，数据库，或者提交到远程服务器。
Sink 在设置存储数据时，可以向文件系统中，数据库中，hadoop 中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到 Hadoop 中，便于日后进行相应的数据分析。
** FLUME 介绍
Flume 是一个分布式、可靠、和高可用的海量日志聚合的系统，支持在系统中定制各类数据发送方，用于收集数据；同时，Flume 提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。
设计目标：
(1) 可靠性
当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume 提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据 agent 首先将 event 写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是 scribe 采用的策略，当数据接收方 crash 时，将数据写到本地，待恢复后，继续发送），Best effort（数据发送到接收方后，不会进行确认）。
(2) 可扩展性
Flume 采用了三层架构，分别为 agent，collector 和 storage，每一层均可以水平扩展。其中，所有 agent 和 collector 由 master 统一管理，这使得系统容易监控和维护，且 master 允许有多个（使用 ZooKeeper 进行管理和负载均衡），这就避免了单点故障问题。
(3) 可管理性
所有 agent 和 colletor 由 master 统一管理，这使得系统便于维护。多 master 情况，Flume 利用 ZooKeeper 和 gossip，保证动态配置数据的一致性。用户可以在 master 上查看各个数据源或者数据流执行情况，且可以对各个数据源配置和动态加载。Flume 提供了 web 和 shell script command 两种形式对数据流进行管理。
(4) 功能可扩展性
用户可以根据需要添加自己的 agent，collector 或者 storage。此外，Flume 自带了很多组件，包括各种 agent（file，syslog 等），collector 和 storage（file，HDFS 等）。
** Flume 的 一些核心概念：
Agent 代理	使用 JVM 运行 Flume。每台机器运行一个 agent，但是可以在一个 agent 中包含多个 sources 和 sinks。
Client 客户端	生产数据，运行在一个独立的线程。
Source 源	从 Client 收集数据，传递给 Channel。
Sink 接收器	从 Channel 收集数据，进行相关操作，运行在一个独立线程。
Channel 通道	连接 sources 和 sinks，这个有点像一个队列。
Events 事件	传输的基本数据负载。
* kafka
[[http://www.infoq.com/cn/articles/kafka-analysis-part-1][Kafka 剖析（一）：Kafka 背景及架构介绍]]
[[http://www.infoq.com/cn/articles/apache-kafka/][Apache Kafka：下一代分布式消息系统]]
Kafka 保证一个 Partition 内的消息的有序性。
Producer 使用 push 模式将消息发布到 broker，Consumer 使用 pull 模式从 broker 订阅并消费消息。
*** kafka 架构
Broker
Kafka 集群包含一个或多个服务器，这种服务器被称为 broker

Topic
每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）

Partition
Partition 是物理上的概念，每个 Topic 包含一个或多个 Partition.

Producer
负责发布消息到 Kafka broker

Consumer
消息消费者，向 Kafka broker 读取消息的客户端。

Consumer Group
每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）。

Topic 在逻辑上可以被认为是一个 queue, 每条消费都必须指定它的 Topic，可以简单理解为必须指明把这条消息放进哪个 queue 里。为了使得 Kafka 的吞吐率可以线性提高，物理上把 Topic 分成一个或多个 Partition，每个 Partition 在物理上对应一个文件夹，该文件夹下存储这个 Partition 的所有消息和索引文件。
*** kafka 组的作用
Consumer Group
使用 Consumer high level API 时，同一 Topic 的一条消息只能被同一个 Consumer Group 内的一个 Consumer 消费，但多个 Consumer Group 可同时消费这一消息。

这是 Kafka 用来实现一个 Topic 消息的广播（发给所有的 Consumer）和单播（发给某一个 Consumer）的手段。一个 Topic 可以对应多个 Consumer Group。如果需要实现广播，只要每个 Consumer 有一个独立的 Group 就可以了。要实现单播只要所有的 Consumer 在同一个 Group 里。用 Consumer Group 还可以将 Consumer 进行自由的分组而不需要多次发送消息到不同的 Topic。
* hadoop
** hadoop map reduce 数量
map 的数量通常是由 hadoop 集群的 DFS 块大小确定的，也就是输入文件的总块数，正常的 map 数量的并行规模大致是每一个 Node 是 10~100 个，对于 CPU 消耗较小的作业可以设置 Map 数量为 300 个左右，但是由于 hadoop 的没一个任务在初始化时需要一定的时间，因此比较合理的情况是每个 map 执行的时间至少超过 1 分钟。
InputFormat 在默认情况下会根据 hadoop 集群的 DFS 块大小进行分片，每一个分片会由一个 map 任务来进行处理，当然用户还是可以通过参数 mapred.min.split.size 参数在作业提交客户端进行自定义设置。
Map 任务的个数也能通过使用 JobConf 的 conf.setNumMapTasks(int num)方法来手动地设置。
同样，Reduce 任务也能够与 map 任务一样，通过设定 JobConf 的 conf.setNumReduceTasks(int num)方法来增加任务个数。
** hadoop rm 后如何修复
　hadoop 的 hdfs 中被删除文件的恢复原理和回收站原理是一样的，就是在删除 hdfs 文件时，被删除的文件被移动到了 hdfs 的.Trash 文件夹中，恢复时只需将该文件夹中文件拿出即可。
1 设置.Trash 文件夹

    如果需要恢复 hdfs 中文件，就需要设置.Trash，hadoop 的.Trash 默认是关闭的。具体设置如下：

    <property>
          <name>fs.trash.interval</name>
          <value>10080</value>
          If zero, the trash feature is disabled.    
    </property>

    该配置项在 core-site.xml 中，fs.trash.interval 代表删除的文件保留的时间，时间单位为分钟，默认为 0 代表不保存删除的文件。我们只需要设置该时间即可打开.Trash。
2 设置后删除文件会显示删除的文件被移动到了 hdfs://192.168.1.100:9000/user/hadoop/.Trash/Current 中，举例如下：

    14/10/20 16:48:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 10080 minutes, Emptier interval = 0 minutes.
    Moved: 'hdfs://10.207.0.217:9000/test_out/mr-20141017111556' to trash at: hdfs://192.168.1.100:9000/user/hadoop/.Trash/Current

3 恢复时只需要将.Trash 中文件移动到我们设置的目录即可，例如将.Trash 中 test1 文件移动到/test 目录，

    bin/hadoop fs -mv /user/hadoop/.Trash/Current/test1 /test
** yarn 架构
file:./Pictures/HadoopPictures/yarn1.jpg
ResourceManager 管理所有应用程序计算资源的分配，每一个应用的 ApplicationMaster 负责相应的调度和协调。一个应用程序无非是一个单独的传统的 MapReduce 任务或者是一个 DAG( 有向无环图 ) 任务。ResourceManager 和每一台机器的节点管理服务器能够管理用户在那台机器上的进程并能对计算进行组织。NodeManager 是每一台机器框架的代理，是执行应用程序的容器，监控应用程序的资源使用情况 (CPU，内存，硬盘，网络 ) 并且向调度器汇报。
* hbase 架构
HBase 采用 Master/Slave 架构搭建集群，它隶属于 Hadoop 生态系统，由一下类型节点组成：HMaster 节点、HRegionServer 节点、ZooKeeper 集群，而在底层，它将数据存储于 HDFS 中，因而涉及到 HDFS 的 NameNode、DataNode 等
* spark
** rdd
RDD 是 Spark 的核心，也是整个 Spark 的架构基础。它的特性可以总结如下：
它是不变的数据结构存储
它是支持跨集群的分布式数据结构
可以根据数据记录的 key 对结构进行分区
提供了粗粒度的操作，且这些操作都支持分区
它将数据存储在内存中，从而提供了低延迟性

在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。
** Spark 运行架构
Client：客户端进程，负责提交作业到 Master。

Master：Standalone 模式中主控节点，负责接收 Client 提交的作业，管理 Worker，并命令 Worker 启动 Driver 和 Executor。

Application：Spark Application 的概念和 Hadoop MapReduce 中的类似，指的是用户编写的 Spark 应用程序，包含了一个 Driver 功能的代码和分布在集群中多个节点上运行的 Executor 代码；

Driver：Spark 中的 Driver 即运行上述 Application 的 main()函数并且创建 SparkContext，其中创建 SparkContext 的目的是为了准备 Spark 应用程序的运行环境。在 Spark 中由 SparkContext 负责和 ClusterManager 通信，进行资源的申请、任务的分配和监控等；当 Executor 部分运行完毕后，Driver 负责将 SparkContext 关闭。通常用 SparkContext 代表 Drive；

Executor：Application 运行在 Worker 节点上的一个进程，该进程负责运行 Task，并且负责将数据存在内存或者磁盘上，每个 Application 都有各自独立的一批 Executor。在 Spark on Yarn 模式下，其进程名称为 CoarseGrainedExecutorBackend，类似于 Hadoop MapReduce 中的 YarnChild。一个 CoarseGrainedExecutorBackend 进程有且仅有一个 executor 对象，它负责将 Task 包装成 taskRunner，并从线程池中抽取出一个空闲线程运行 Task。每个 CoarseGrainedExecutorBackend 能并行运行 Task 的数量就取决于分配给它的 CPU 的个数了；

Cluster Manager：指的是在集群上获取资源的外部服务，目前有：

Standalone：Spark 原生的资源管理，由 Master 负责资源的分配；
Hadoop Yarn：由 YARN 中的 ResourceManager 负责资源的分配；
Worker：集群中任何可以运行 Application 代码的节点，类似于 YARN 中的 NodeManager 节点。在 Standalone 模式中指的就是通过 Slave 文件配置的 Worker 节点，在 Spark on Yarn 模式中指的就是 NodeManager 节点；

作业（Job）：包含多个 Task 组成的并行计算，往往由 Spark Action 催生，一个 JOB 包含多个 RDD 及作用于相应 RDD 上的各种 Operation；

阶段（Stage）：每个 Job 会被拆分很多组 Task，每组任务被称为 Stage，也可称 TaskSet，一个作业分为多个阶段；

任务（Task）： 被送到某个 Executor 上的工作任务；

DAGScheduler： 实现将 Spark 作业分解成一到多个 Stage，每个 Stage 根据 RDD 的 Partition 个数决定 Task 的个数，然后生成相应的 Task set 放到 TaskScheduler 中。

TaskScheduler：实现 Task 分配到 Executor 上执行。

SparkContext：整个应用的上下文，控制应用的生命周期。

RDD：Spark 的基本计算单元，一组 RDD 可形成执行的有向无环图 RDD Graph。

SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。
SparkEnv 内创建并包含如下一些重要组件的引用:

MapOutPutTracker**：负责 Shuffle 元信息的存储。
BroadcastManager**：负责广播变量的控制与元信息的存储。
BlockManager**：负责存储管理、创建和查找块。
MetricsSystem**：监控运行时性能指标信息。
SparkConf**：负责存储配置信息。
Spark 架构
file:./Pictures/SparkPictures/spark1.png
Spark 架构采用了分布式计算中的 Master-Slave 模型。Master 是对应集群中的含有 Master 进程的节点，Slave 是集群中含有 Worker 进程的节点。Master 作为整个集群的控制器，负责整个集群的正常运行；Worker 相当于是计算节点，接收主节点命令与进行状态汇报；Executor 负责任务的执行；Client 作为用户的客户端负责提交应用，Driver 负责控制一个应用的执行。

Spark 集群部署后，需要在主节点和从节点分别启动 Master 进程和 Worker 进程，对整个集群进行控制。在一个 Spark 应用的执行过程中，Driver 和 Worker 是两个重要角色。Driver 程序是应用逻辑执行的起点，负责作业的调度，即 Task 任务的分发，而多个 Worker 用来管理计算节点和创建 Executor 并行处理任务。在执行阶段，Driver 会将 Task 和 Task 所依赖的 file 和 jar 序列化后传递给对应的 Worker 机器，同时 Executor 对相应数据分区的任务进行处理。

Spark 的整体流程为：Client 提交应用，Master 找到一个 Worker 启动 Driver，Driver 向 Master 或者资源管理器申请资源，之后将应用转化为 RDD Graph，再由 DAGScheduler 将 RDD Graph 转化为 Stage 的有向无环图提交给 TaskScheduler，由 TaskScheduler 提交任务给 Executor 执行。在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。
在集群启动的时候，各个 slave 节点（也可以说是 worker）会向集群的 Master 注册，告诉 Master 我随时可以干活了，随叫随到
Master 会根据一种心跳机制来实时监察集群中各个 worker 的状态，是否能正常工作
Driver Application 提交作业的时候也会先向 Master 注册信息
作业注册完毕之后，Master 会向 worker 发射 Executor 命令
worker 产生若干个 Executor 准备执行
各个 worker 中的 Executor 会向 Driver Application 注册 Executor 信息，以便 Driver Application 能够将作业分发到具体的 Executor
Executor 会定期向 Driver Application 报告当前的状态更新信息
Driver Application 发射任务到 Executor 执行
* 有一个整数， 如何得到质数的乘机得到这个整数。
程序分析：对 n 进行分解质因数，应先找到一个最小的质数 k，然后按下述步骤完成： 
(1)如果这个质数恰等于 n，则说明分解质因数的过程已经结束，打印出即可。
(2)如果 n<>k，但 n 能被 k 整除，则应打印出 k 的值，并用 n 除以 k 的商,作为新的正整数你 n,
　重复执行第一步。
(3)如果 n 不能被 k 整除，则用 k+1 作为 k 的值,重复执行第一步。

#+BEGIN_SRC C
  #include "stdio.h"
  #include "conio.h"
  main()
  {
    int n,i;
    printf("\nplease input a number:\n");
    scanf("%d",&n);
    printf("%d=",n);
    for(i=2;i<=n;i++)
      while(n!=i)
      {
        if(n%i==0)
        {
          printf("%d*",i);
          n=n/i;
        }
        else
          break;
      }
    printf("%d",n);
    getch();
  }
#+END_SRC

* hive
**  hive 优化
*** 参数设置调优
**** 设置 jvm 重用
  JVM 重用是 hadoop 调优参数的内容，对 hive 的性能具有非常大的影响，特别是对于很难避免小文件的场景或者 task 特别多的场景，这类场景大多数执行时间都很短。
  1. JVM 重用可以使得 JVM 实例在同一个 JOB 中重新使用 N 次，N 的值可以在 Hadoop 的 mapre-site.xml 文件中进行设置
  mapred.job.reuse.jvm.num.tasks
  2. 也可在 hive 的执行设置：
  set  mapred.job.reuse.jvm.num.tasks=10;
**** 开启并行执行
     hive 执行开启：set hive.exec.parallel=true
**** 调整 reducer 个数
     设置  hive.exec.reducers.bytes.per.reducer（默认为 1GB），受 hive.exec.reducers.max（默认为 999）影响：

     mapred.reduce.tasks = min ( 参数 2，总输入数据量/参数 1 )
***  count(distinct)的优化
    /*改写前*/
    select a，count(distinct b) as c from tbl group by a;
    /*改写后*/
    select a，count(*) as c
    from (select distinct a，b from tbl) group by a;
*** 数据倾斜问题
    操作：join,group by,count distinct

    原因：key 分布不均匀

    倾斜度衡量：平均记录数超过 50w 且最大记录数是超过平均记录数的 4 倍；最长时长比平均时长超过 4 分钟，且最大时长超过平均时长的 2 倍

    数据倾斜 Hive 的典型操作是： 多表关联的查询，SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)

**** Reducer 端数据倾斜。
    如果一个 Reducer 要处理的数据量远多于其它 Reducer 要处理的数据量，那么就会产生 Reducer 端的数据倾斜。那么 Reducer 要处理的数据量是如何确定的呢？通常数据(KV 数值对）Shuffle 到某个 Reducer 是根据 Key 进行 Hash 然后对 Reducer 个数进行取模。

    那么 Reducer 端的优化包含三种做法
    1. 增加 Reducer 个数
    2. 空 KEY 过滤（join 的时候）
    3. 空 KEY 转换(left outer join 的时候)
**** 设置 set hive.groupby.skewindata=true
     这个参数的意思是做 Reduce 操作的时候，拿到的 key 并不是所有相同值给同一个 Reduce，而是随机分发，然后 Reduce 做聚合，做完之后再做一轮 MR，拿前面聚合过的数据再算结果。
**** 大小表关联，大表和大表关联
***** 大小表关联
      将 key 相对分散，并且数据量小的表放在 join 的左边
***** 大表和大表关联
      大表和大表关联：把空值的 key 变成一个字符串加上随机数，把倾斜的数据分到不同的 reduce 上，由于 null 值关联不上，处理后并不影响最终结果。
      所以这个参数其实跟 Hive.Map.aggr 做的是类似的事情，只是拿到 Reduce 端来做，而且要额外启动一轮 Job，所以 *其实不怎么推荐用，效果不明显* 。
***** 大表和不小的表关联
      优化 select * from log a left outer join members b on a.memberid = b.memberid

      优化后：（先过滤一部分， 让表更小， 再按小表关联大表）
      #+BEGIN_SRC sql
        select /*+mapjoin(x)*/* from log a left outer join
            ( select  /*+mapjoin(c)*/d.*
              from (select  distinct memberid from log ) c
              join members d
              on c.memberid = d.memberid
            )x on a.memberid = b.memberid
      #+END_SRC

    倾斜分成 group by 造成的倾斜和 join 造成的倾斜，需要分开看。

    1. group by
        group by 造成的倾斜有两个参数可以解决，一个是 Hive.Map.aggr，默认值已经为 true，意思是会做 Map 端的 combiner。另一个参数是 Hive.groupby. skewindata。这个参数的意思是做 Reduce 操作的时候，拿到的 key 并不是所有相同值给同一个 Reduce，而是随机分发，然后 Reduce 做聚合，做完之后再做一轮 MR，拿前面聚合过的数据再算结果。所以这个参数其实跟 Hive.Map.aggr 做的是类似的事情，只是拿到 Reduce 端来做，而且要额外启动一轮 Job，所以 *其实不怎么推荐用，效果不明显* 。

        如果说要改写 SQL 来优化的话，可以按照下面这么做：
        /*改写前*/
        select a，count(distinct b) as c from tbl group by a;
        /*改写后*/
        select a，count(*) as c
        from (select distinct a，b from tbl) group by a;

    2. join
       [[http://blog.csdn.net/wf1982/article/details/7200376][map join 优化]]
       - common join
         不受数据量大小影响， 但是最没有效率。

       - map join
         map join 计算步骤分为两步： 将小表数据变成 hashtable 广播到所有的 map 端， 将大表的数据进行合理的切分， 然后在 map 阶段的时候用大表的数据一行一行的去探测(probe)小表的 hashtable. 如果 join key 相等， 就写入到 hdfs.
         缺点： 小表有大小限制。

       - bucket map join
         当连接的两个表的 join key 都是 bucket column 的时候， 就可以通过：hive.optimize.bucketmapjoin=ture 来控制 hive 执行 bucket map join.
         注意: 小表的 number buckets 必须是大表的数倍.
         bucket map join 的执行也分为两步： 
         1. 先将小表做 map 操作变成 hashtable,然后广播到大表的 map 端，大表接受了 num_buckets 个小表的 hashtable 并不需要合并称一个大的 hashtable。

         2. 大表也会产生 num_buckets 个 split， 每个 split 标记跟小表的标记一致。这样只需要把小表的一个 hashtable 放入内存即可。
         
         缺点： 内存限制为小表中最大的那个 hashtable 的大小。 
       - sort merge bucket map join
         Bucket Map Join 并没有解决 map join 在小表必须完全装载进内存的限制, 如果想要在一个 reduce 节点的大表和小表都不用装载进内存，必须使两个表都在 join key 上有序才行，你可以在建表的时候就指定 sorted byjoin key 或者使用 index 的方式.

            set hive.optimize.bucketmapjoin = true;

            set hive.optimize.bucketmapjoin.sortedmerge = true;

            set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

            Bucket columns == Join columns == sort columns

            这样小表的数据可以每次只读取一部分，然后还是用大表一行一行的去匹配，这样的 join 没有限制内存的大小. 并且也可以执行全外连接.
       - skew join
            真实数据中数据倾斜是一定的, hadoop 中默认是使用

            hive.exec.reducers.bytes.per.reducer = 1000000000

            也就是每个节点的 reduce 默认是处理 1G 大小的数据，如果你的 join 操作也产生了数据倾斜，那么你可以在 hive 中设定

            set hive.optimize.skewjoin = true; 
            set hive.skewjoin.key = skew_key_threshold（default = 100000）

            hive 在运行的时候没有办法判断哪个 key 会产生多大的倾斜，所以使用这个参数控制倾斜的阈值，如果超过这个值，新的值会发送给那些还没有达到的 reduce, 一般可以设置成你

            （处理的总记录数/reduce 个数）的 2-4 倍都可以接受.

            倾斜是经常会存在的，一般 select 的层数超过 2 层，翻译成执行计划多于 3 个以上的 mapreduce job 都很容易产生倾斜，建议每次运行比较复杂的 sql 之前都可以设一下这个参数. 如果你不知道设置多少，可以就按官方默认的 1 个 reduce 只处理 1G 的算法，那么  skew_key_threshold  = 1G/平均行长. 或者默认直接设成 250000000 (差不多算平均行长 4 个字节)
*** hive 如何很多任务用一张表的时候 这些任务起一次 map(multi insert)
** hive 更新数据
   hive 不支持对某个具体行的操作，hive 对数据的操作只支持覆盖原数据和追加数据。对于更新操作，hive 是通过查询将原表的数据进行转化最后存储在新表里，这和传统数据库的更新操作有很大不同。
* hadoop
** 数据导入到 hadoop
   业务库 mysql 数据量比较大， 如何同步到 hadoop？比如千万级，亿级数据。

** MapR 的 join 如何写

** mysql 到 hadoop 的更新数据如何同步

* java
** hashmap 的结构
   在我们编程的世界里数据的基本组织可以说有三种形式。
    1. 结构体(或对象)
    2. 数组
    3. 链表

   hashmap 数组加链表(链表散列)
* mahout
** 推荐系统的评测指标
为了评估推荐算法的好坏需要各方面的评估指标。

1. 准确率
准确率就是最终的推荐列表中有多少是推荐对了的。

2. 召回率
召回率就是推荐对了的占全集的多少。

** 基于物品的协同过滤
推荐系统应用数据分析技术，找出用户最可能喜欢的东西推荐给用户，现在很多电子商务网站都有这个应用。目前用的比较多、比较成熟的推荐算法是协同过滤（Collaborative Filtering，简称 CF）推荐算法，CF 的基本思想是根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。
*** 1
   三、Item-based 算法详细过程

  （1）相似度计算
  Item-based 算法首选计算物品之间的相似度，计算相似度的方法有以下几种：

  1. 基于余弦（Cosine-based）的相似度计算，通过计算两个向量之间的夹角余弦值来计算物品之间的相似性，公式如下：
  其中分子为两个向量的内积，即两个向量相同位置的数字相乘。

  2. 基于关联（Correlation-based）的相似度计算，计算两个向量之间的 Pearson-r 关联度，公式如下：
  其中表示用户 u 对物品 i 的打分，表示第 i 个物品打分的平均值。

  3. 调整的余弦（Adjusted Cosine）相似度计算，由于基于余弦的相似度计算没有考虑不同用户的打分情况，可能有的用户偏向于给高分，而有的用户偏向于给低分，该方法通过减去用户打分的平均值消除不同用户打分习惯的影响，公式如下：
  其中表示用户 u 打分的平均值。

  （2）预测值计算
  根据之前算好的物品之间的相似度，接下来对用户未打分的物品进行预测，有两种预测方法：
  1. 加权求和。
  通过对用户 u 已打分的物品的分数进行加权求和，权值为各个物品与物品 i 的相似度，然后对所有物品相似度的和求平均，计算得到用户 u 对物品 i 打分，公式如下：
  其中为物品 i 与物品 N 的相似度，为用户 u 对物品 N 的打分。
  2. 回归。
  和上面加权求和的方法类似，但回归的方法不直接使用相似物品 N 的打分值，因为用余弦法或 Pearson 关联法计算相似度时存在一个误区，即两个打分向量可能相距比较远（欧氏距离），但有可能有很高的相似度。因为不同用户的打分习惯不同，有的偏向打高分，有的偏向打低分。如果两个用户都喜欢一样的物品，因为打分习惯不同，他们的欧式距离可能比较远，但他们应该有较高的相似度。在这种情况下用户原始的相似物品的打分值进行计算会造成糟糕的预测结果。通过用线性回归的方式重新估算一个新的值，运用上面同样的方法进行预测。重新计算的方法如下：
  其中物品 N 是物品 i 的相似物品，和通过对物品 N 和 i 的打分向量进行线性回归计算得到，为回归模型的误差。具体怎么进行线性回归文章里面没有说明，需要查阅另外的相关文献。

*** 2
    基于物品的协同过滤
ItemBasedCF 应该是业界的应用最广泛的推荐算法了。该算法的核心思想主要是：给目标用户推荐与他喜欢的物品相似度较高高的物品。我们经常在京东、天猫上看到「购买了该商品的用户也经常购买的其他商品」，就是主要基于 ItemBasedCF。一般我们先计算物品之间的相似度，然后根据物品的相似度和用户的历史行为给用户生成推荐列表。

物品 i 和 j 之间的相似度可以使用如下公式计算：

[Math Processing Error]
从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是说每个用户都可以通过他们的历史兴趣列表给物品“贡献”相似度。

根据上述核心思想，可以有如下算法步骤：

建立用户-物品的倒排表
物品与物品之间的共现矩阵 C[i][j]，表示物品 i 与 j 共同被多少用户所喜欢。
用户与用户之间的相似度矩阵 W[i][j] ， 根据上述相似度计算公式计算。
用上面的相似度矩阵来给用户推荐与他所喜欢的物品相似的其他物品。用户 u 对物品 j 的兴趣程度可以估计为


[Math Processing Error] 为和物品 j 最相似的前 K 个物品， [Math Processing Error] 为对用户 u 所喜欢的物品集合，W[j][i] 为物品 j 和物品 i 之间的相似度， [Math Processing Error] 为用户 u 对物品 i 的兴趣。

下面是 ItemBasedCF 的代码实现：

class ItemBasedCF:
    def __init__(self,train_file,test_file):
        self.train_file = train_file
        self.test_file = test_file
        self.readData()
    def readData(self):
        #读取文件，并生成用户-物品的评分表和测试集
        self.train = dict()     #用户-物品的评分表
        for line in open(self.train_file):
            # user,item,score = line.strip().split(",")
            user,item,score,_ = line.strip().split("\t")
            self.train.setdefault(user,{})
            self.train[user][item] = int(score)
        self.test = dict()      #测试集
        for line in open(self.test_file):
            # user,item,score = line.strip().split(",")
            user,item,score,_ = line.strip().split("\t")
            self.test.setdefault(user,{})
            self.test[user][item] = int(score)

    def ItemSimilarity(self):
        #建立物品-物品的共现矩阵
        C = dict()  #物品-物品的共现矩阵
        N = dict()  #物品被多少个不同用户购买
        for user,items in self.train.items():
            for i in items.keys():
                N.setdefault(i,0)
                N[i] += 1
                C.setdefault(i,{})
                for j in items.keys():
                    if i == j : continue
                    C[i].setdefault(j,0)
                    C[i][j] += 1
        #计算相似度矩阵
        self.W = dict()
        for i,related_items in C.items():
            self.W.setdefault(i,{})
            for j,cij in related_items.items():
                self.W[i][j] = cij / (math.sqrt(N[i] * N[j]))
        return self.W

    #给用户 user 推荐，前 K 个相关用户
    def Recommend(self,user,K=3,N=10):
        rank = dict()
        action_item = self.train[user]     #用户 user 产生过行为的 item 和评分
        for item,score in action_item.items():
            for j,wj in sorted(self.W[item].items(),key=lambda x:x[1],reverse=True)[0:K]:
                if j in action_item.keys():
                    continue
                rank.setdefault(j,0)
                rank[j] += score * wj
        return dict(sorted(rank.items(),key=lambda x:x[1],reverse=True)[0:N])
采用 MovieLens 数据集对 ItemCF 算法测试之后各评测指标的结果如下


UserCF 和 ItemCF 的区别和应用

UserCF 算法的特点是：
    用户较少的场合，否则用户相似度矩阵计算代价很大
    适合时效性较强，用户个性化兴趣不太明显的领域
    对新用户不友好，对新物品友好，因为用户相似度矩阵不能实时计算
    很难提供令用户信服的推荐解释

对应地，ItemCF 算法的特点：
    适用于物品数明显小于用户数的场合，否则物品相似度矩阵计算代价很大
    适合长尾物品丰富，用户个性化需求强的领域
    对新用户友好，对新物品不友好，因为物品相似度矩阵不需要很强的实时性
    利用用户历史行为做推荐解释，比较令用户信服
    因此，可以看出 UserCF 适用于物品增长很快，实时性较高的场合，比如新闻推荐。而在图书、电子商务和电影领域，比如京东、天猫、优酷中，ItemCF 则能极大地发挥优势。在这些网站中，用户的兴趣是比较固定和持久的，而且这些网站的物品更新速度不会特别快，一天一更新是在忍受范围内的。
** mahout 中的算法
   [[http://blog.csdn.net/u010967382/article/details/39183839][Mahout 推荐算法编程实践]]
*** mahout 推荐算法
    Perference：表示用户的喜好数据，是个三元组（userid, itemid, value），分别表示用户 id, 物品 id 和用户对这个物品的喜好值。
    DataModel：是 Perference 的集合，可以认为是协同过滤用到的 user*item 的大矩阵。DateModel 可以来自 db, 文件或者内存。
    Similarity：相似度计算的接口，各种相似度计算算法都是继承自这个接口，具体相似度计算的方法，可以参考这篇文章：http://anylin.iteye.com/blog/1721978
    Recommender: 利用 Similarity 找到待推荐 item 集合后的各种推荐策略，这是最终要暴露个使用者的推荐接口
    Item-based:
            GenericItemBasedRecommender
            GenericBooleanPrefItemBasedRecommender
            KnnItemBasedRecommender
    User-based:
            GenericUserBasedRecommender
            GenericBooleanPerfUserBasedRecommender
    Model-based:
            SlopeOneRecommender
            SVDRecommender
            TreeClusteringRecommender
    ItemAverageRecommender
            ItemUserAverageRecommender
* 数据分析维度
** 店铺的维度
   时间， 平台维度（京东， 淘宝，一号店，亚马逊等），流量来源维度（移动，pc 端），地域维度（省，市），行业（服饰，影像数码）

** 会员
   购买时间维度， 地域，平台， 流量来源

** 会员 rfm 模型
   最近一次消费，消费频率， 消费金额

** 商品
   地域，平台， 时间，
** 用户画像
*** user profile
    性别， 年龄， 地域， 婚否， 孕妇， 小孩年龄， 星座， 收入
*** 用户行为
    最近活跃时间， 活跃程度, 购买金额
*** 购买偏好
    购买类型分类： 品牌敏感， 促销敏感型， 购买力旺盛型。
*** 用户标签
    网购达人， 奶爸奶妈， 数码达人， 家庭夫妇， 时尚男女
* python
** python 一二维数组转换
   一维->二维
   #+BEGIN_SRC java
     int[] a = new int[n];
     int[,] b = new int[r, c];

     for (int i = 0; i < n; i++)
         b[i / c, i % c] = a[i];
or
    for (int i = 0; i < r; i++)
    {
        for (int j = 0; j < c; j++)
        {
            b[i, j] = a[i * c + j];
        }
    }
   #+END_SRC

   二维->一维
   #+BEGIN_SRC java
     int[,] a = new int[r, c];
     int[] b = new int[r * c];
     for(int i = 0; i < b.Length; i++)
         b[i] = a[i / c, i % c];
   #+END_SRC
