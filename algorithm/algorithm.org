#+OPTIONS: ^:nil、
* 简介
1. 算法：通俗而言，算法是一个定义明确的计算过程，可以一些值或一组值作为输入并产生一些值或一组值作为输出。因此算法就是将输入转为输出的一系列计算步骤。
2. 算法的三大特征：
   + 有限
   + 指令明确
   + 有效
* 数据结构与算法
  仅需要常量空间并以线性时间运行的联机算法几乎是完美的算法.
  对数出现的规律：如果一个算法用常数时间将问题的大小削减为其一部分（通常为 1/2）,那么算法就是 log(N).
  任何仅通过使用比较来进行排序的算法在最坏的情况下只需要 Omega(NlogN)次比较。
  gcd 算法(两个整数的最大公因数)和求幂算法应用在密码学中.
** tree
** graph
* 统治世界的十大算法
[[http://36kr.com/p/212499.html][统治世界的十大算法]]
1. 归并排序、快速排序及堆积排序
2. 傅里叶变换与快速傅里叶变换
3. 迪杰斯特拉（Dijkstra）算法
4. RSA 算法
5. 安全哈希算法（SHA）
6. 整数因子分解
7. 链接分析
8. 比例积分微分算法
9. 数据压缩算法
10. 随机数生成算法
* 排序算法
[[http://blog.csdn.net/xiazdong/article/details/8462393][九大排序算法再总结]]
[[http://wuchong.me/blog/2014/02/09/algorithm-sort-summary/][经典排序算法总结与实现]]
[[https://zh.wikipedia.org/wiki/%25E6%258E%2592%25E5%25BA%258F%25E7%25AE%2597%25E6%25B3%2595][排序算法 维基百科]]
[[https://en.wikipedia.org/wiki/Sorting_algorithm][sorting algorithm wikipeida]]
时间复杂度, 空间复杂度, 稳定性.

*穩定性* ：穩定排序算法會讓原本有相等鍵值的紀錄維持相對次序。也就是如果一個排序算法是穩定的，當有兩個相等鍵值的紀錄 R 和 S，且在原本的串列中 R 出現在 S 之前，在排序過的串列中 R 也將會是在 S 之前。
不穩定排序算法可能會在相等的鍵值中改變紀錄的相對次序，但是穩定排序算法從來不會如此。
** 稳定的排序
*** 冒泡排序 Bubble Sort

#+BEGIN_SRC screen  
// 虚拟码
  function bubble_sort (array, length) {
      var i, j;
      for(i from 0 to length-1){
          for(j from 0 to length-1-i){
              if (array[j] > array[j+1])
                  swap(array[j], array[j+1])
          }
      }
  }
#+END_SRC


#+BEGIN_SRC java

    public static void bubble_sort(int[] arr) {
      int i, j, temp, len = arr.length;
      for (i = 0; i < len - 1; i++)
        for (j = 0; j < len - 1 - i; j++)
          if (arr[j] > arr[j + 1]) {
            temp = arr[j];
            arr[j] = arr[j + 1];
            arr[j + 1] = temp;
          }
    }
#+END_SRC


#+BEGIN_SRC scala

    def bubbleSort(array: Array[Int]): Unit = {
      val len = array.length
      for (i <- 0 until len) {
        for (j <- 0 until len - i - 1) {
          if (array(j) > array(j + 1)){
            val temp = array(j)
            array(j) = array(j + 1)
            array(j + 1) = temp
          }
        }
      }
    }
#+END_SRC


* 机器学习十大算法简介
[[https://github.com/ty4z2008/Qix/blob/master/dl.md][机器学习(Machine Learning)&深度学习(Deep Learning)资料]]

[[https://www.coursera.org/learn/machine-learning][Andrew Ng’s free Machine Learning class on Coursera]]
* A Deep Learning Tutorial: From Perceptrons to Deep Networks
  https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks

** Perceptrons: Early Deep Learning Algorithms
   线性模型, 不能表示非线性可分的任务, 比如异或关系(xor)

** Feedforward Neural Networks for Deep Learning
   A linear composition of a bunch of linear functions is still just a linear function, so most neural networks use non-linear activation functions.

   Because of this, most neural networks use non-linear activation functions like the logistic, tanh, binary or rectifier. Without them the network can only learn functions which are linear combinations of its inputs.

   The Problem with Large Networks, increasing the number of hidden layers leads to two known issues:
     1. Vanishing gradients: as we add more and more hidden layers, backpropagation becomes less and less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the networks.
     2. Overfitting: perhaps the central problem in machine learning. Briefly, overfitting describes the phenomenon of fitting the training data too closely, maybe with hypotheses that are too complex. In such a case, your learner ends up fitting the training data really well, but will perform much, much more poorly on real examples.

** Autoencoders
   An autoencoder is typically a feedforward neural network which aims to learn a compressed, distributed representation (encoding) of a dataset.

** “深度学习”和“多层神经网络”的区别?
    https://www.zhihu.com/question/26017374
   简单来说，原来多层神经网络做的步骤是：特征映射到值。特征是人工挑选。

   深度学习做的步骤是 信号->特征->值。 特征是由网络自己选择。


    作者：Bipolar Bear
    链接：https://www.zhihu.com/question/26017374/answer/127924427
    来源：知乎
    著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

    以我的理解，两种网络被设计出来，所要解决的问题和目的不同。
    多层神经网络与 universal approximation theorem [1] （泛逼近性原理，不知这样翻译可对？）相伴而生。该理论指出，单隐藏层（hidden layer）非线性前馈神经网络，可以在实数空间近似任何连续函数。上世纪 80 90 年代，Backpropagation 刚刚开始大行其道，利用这一算法，只需知道输入和输出便可训练网络参数，从而得到一个神经网络“黑箱”。之所以称为黑箱，是因为无需知道 y=f(x) 中 f 的表达式是什么，也能轻易做函数计算，因为 f（objective function）就是网络本身。多层神经网络的座右铭是：“函数是什么我不管，反正我能算！“。
    当然多层神经网络并非天下无敌，它有三个主要限制：
      1. 是在面对大数据时，需要人为提取原始数据的特征作为输入，这个问题前面的知友提到过@杨延生。必须忽略不相关的变量，同时保留有用的信息。这个尺度很难掌握，多层神经网络会把蹲在屋顶的 Kitty 和骑在猫奴头上的 Kitty 识别为不同的猫咪，又会把二哈和狼归类为同一种动物。前者是对不相关变量过于敏感，后者则因无法提取有实际意义的特征。
      2. 是想要更精确的近似复杂的函数，必须增加隐藏层的层数，这就产生了梯度扩散问题@青青子衿。所谓“强弩之末势不能穿鲁缟“。
      3. 是无法处理时间序列数据（比如音频），因为多层神经网络不含时间参数。随着人工智能需求的提升，我们想要做复杂的图像识别，做自然语言处理，做语义分析翻译，等等。多层神经网络显然力不从心。
    那么深度模型是如何解决以上三个问题的。
      1. 深度学习自动选择原始数据的特征。举一个图像的例子，将像素值矩阵输入深度网络（这里指常用于图像识别的卷积神经网络 CNN），网络第一层表征物体的位置、边缘、亮度等初级视觉信息。第二层将边缘整合表征物体的轮廓……之后的层会表征更加抽象的信息，如猫或狗这样的抽象概念。所有特征完全在网络中自动呈现，并非出自人工设计。更重要的一点是这种随着层的深入，从具象到抽象的层级式表征跟大脑的工作原理吻合，视网膜接收图像从 LGN 到视皮层、颞叶皮层再到海马走的是同样的路数[2]！
      2. 深度网络的学习算法。一种方法是改变网络的组织结构，比如用卷积神经网络代替全连接（full connectivity）网络，训练算法仍依据 Backpropagating gradients 的基本原理。另一种则是彻底改变训练算法，我尝试过的算法有 Hessian-free optimization[3]，recursive least-squares(RLS) 等。
      3. 使用带反馈和时间参数的 Recurrent neural network 处理时间序列数据。
      从某种意义上讲，Recurrent neural network 可以在时间维度上展开成深度网络，可以有效处理音频信息（语音识别和自然语言处理等），或者用来模拟动力系统。本人计算神经科学小博士，写些粗浅的认识，有不对的地方还请各位大神指正。
* 动态规划(Dynamic programming) 求斐波那契
动态规划（英语：Dynamic programming，简称 DP）是一种在数学、管理科学、计算机科学、经济学和生物信息学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。
动态规划常常适用于有重叠子问题[1]和最优子结构性质的问题，动态规划方法所耗时间往往远少于朴素解法。

适用情况:
1. 最优子结构性质。如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态規劃算法解决问题提供了重要线索。
2. 无后效性。即子问题的解一旦确定，就不再改变，不受在这之后、包含它的更大的问题的求解决策影响。
3. 子问题重叠性质。子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态規劃算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。
#+BEGIN_SRC python
  values = {0: 0, 1: 1}

  def fib(n):
      if(not values.has_key(n)):
          value = {n: (fib(n-1) + fib(n-2))}
          values.update(value)
      return values.get(n)
#+END_SRC

* 背包问题
  [[https://zh.wikipedia.org/wiki/%25E8%2583%258C%25E5%258C%2585%25E9%2597%25AE%25E9%25A2%2598][背包问题{维基百科}]]
  背包问题（Knapsack problem）是一种组合优化的 NP 完全问题。问题可以描述为：给定一组物品，每种物品都有自己的重量和价格，在限定的总重量内，我们如何选择，才能使得物品的总价格最高。问题的名称来源于如何选择最合适的物品放置于给定背包中。
  也可以将背包问题描述为决定性问题，即在总重量不超过 W 的前提下，总价值是否能达到 V。
* dataesp
** DATAX-RAY ENSEMBLE
  图像处理器与机器学习结合，一个新的预测性智能分析领域将会被开启—图像识别。
  线性回归模型( Logistic Regression)、神经网络( Neural Networks)、限制玻尔兹曼机( Restricted Boltzmann Machines)
** DataX-ray Screening
平台运用包括奇异值分解、k 最近邻分析、关联规则挖掘在内的多种高级机器学习算法。
** DataX-ray Periscope
