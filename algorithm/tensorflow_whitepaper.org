* Introduction
Section 2 describes the programming model and basic concepts of the TensorFlow interface, and Section 3 describes both our single machine and distributed implementations. Section 4 describes several extensions to the basic programming model, and Section 5 describes several optimizations to the basic implementations. Section 6 describes some of our experiences in using TensorFlow, Section 7 describes several programming idioms we have found helpful when using TensorFlow, and Section 9 describes several auxiliary tools we have built around the core TensorFlow system. Sections 10 and 11 discuss future and related work, respectively, and Section 12 offers concluding thoughts.
* Programming Model and Basic Concepts
  A TensorFlow computation is described by a directed graph, which is composed of a set of nodes. 
** Operations and Kernels
   An operation has a name and represents an abstract com- putation (e.g., “matrix multiply”, or “add”). An operation can have attributes, and all attributes must be provided or inferred at graph-construction time in order to instantiate a node to perform the operation.

   A kernel is a particular implementation of an operation that can be run on a particular type of device (e.g., CPU or GPU). 
** Sessions
   Clients programs interact with the TensorFlow system by creating a Session.

   To create a computation graph, the Session interface supports an Extend method to augment the current graph managed by the session with additional nodes and edges (the initial graph when a session is created is empty). 

   Most of our uses of TensorFlow set up a Session with a graph once, and then execute the full graph or a few distinct subgraphs thousands or millions of times via Run calls.
** Variables
   Most tensors do not survive past a single execution of the graph. However, a Variable is a special kind of operation that returns a handle to a persistent mutable tensor that survives across executions of a graph. 
* Implementation
  The main components in a TensorFlow system are the client, which uses the Session interface to communicate with the master, and one or more worker processes, with each worker process responsible for arbitrating access to one or more computational devices (such as CPU cores or GPU cards) and for executing graph nodes on those devices as instructed by the master.

  The distributed implementation shares most of the code with the local implementation, but extends it with support for an environment where the client, the master, and the workers can all be in different processes on different machines. In our distributed environment, these different tasks are containers in jobs managed by a cluster scheduling system.
** Devices
   Devices are the computational heart of TensorFlow. Each worker is responsible for one or more devices, and each device has a device type, and a name. 
** Tensors
   A tensor in our implementation is a typed, multidimensional array. 
** Multi-Device Execution
   Once a system has multiple devices, there are two main complications: deciding which device to place the computation for each node in the graph, and then managing the required communication of data across device boundaries implied by these placement decisions. 
*** Node Placement
    The placement algorithm first runs a simulated execution of the graph. The simulation is described below and ends up picking a device for each node in the graph using greedy heuristics. The node to device placement generated by this simulation is also used as the placement for the real execution.
*** Cross-Device Communication
    Any cross-device edge from x to y is removed and replaced by an edge from x to a new Send node in x’s subgraph and an edge from a corresponding Receive node to y in y’s subgraph. 
*** Distributed Execution
   Distributed execution of a graph is very similar to multi- device execution. After device placement, a subgraph is created per device. Send/Receive node pairs that com- municate across worker processes use remote communi- cation mechanisms such as TCP or RDMA to move data across machine boundaries.
* Extensions
  Using containers, it is possible to share state even across completely disjoint computation graphs associated with different Sessions.
** Gradient Computation
** Partial Execution
** Device Constraints
** Control Flow
** Input Operations
** Queues
** Containers
* Optimizations
* Status and Experience
  Given these circumstances, we found the following strategies critical for porting the Inception model to TensorFlow:
  1. Build tools to gain insight into the exact number of parameters in a given model.
  2. Start small and scale up.
  3. Always ensure that the objective (loss function) matches between machine learning systems when learning is turned off.
  4. Make a single machine implementation match before debugging a distributed implementation.
  5. Guard against numerical errors.
  6. Analyze pieces of a network and understand the magnitude of numerical error.
* Common Programming Idioms
