* 分类
** 机器学习基础
    [[~/note/pictures/1.机器学习基础.png]]
   监督学习一般使用两种类型的目标变量： 标称型（有限目标集中取值） 和 数值型（可以从无限的的数值集合中取值）。
   分类算法可能面临非均衡分类问题： 当训练样本中某个分类的数据多于其他分类数据时， 会产生非均衡分类。

   分类何回归属于监督学习, 因为这类算法必须知道要预测什么， 即要知道目标变量的分类信息。
   无监督学习： 数据没有类别信息， 也不会给定目标值。
   在无监督学习中， 将数据集合分成由类似的对象组成的多个类的过程称为 *聚类* ， 将寻找描述数据统计值的过程称为 *密度估计* 。
   无监督学习可以减少数据维度。

   | 监督学习   <7>                   | 无监督学习            |
   |----------------------------------+-----------------------|
   | K-近邻算法： 线性回归              | K-均值： 最大期望算法 |
   | 朴素贝叶斯算法：局部加权线性回归     | DBSCAN：Parzen窗设计 |
   | 支持向量机： Ridge回归         |                       |
   | 决策树： Lasso最小回归系数估计   |                       |

*** 如何选择合适的算法
    1. 考虑算法的目的
       如果预测目标变量的值， 可以选择监督学习； 否则， 可以选择无监督学习。
    2. 选择监督学习算法之后 --> 确定目标变量的类型
       如果目标变量是离散的， 选择分类器算法； 如果是连续型数值， 选择回归算法。
    3. 选择无监督学习算法之后 --> 确定是否需要将数据划分为离散的组
       如果要划分为离散的组是唯一的需求， 使用聚类算法； 如果还要估计数据与每个分组的相似程度, 使用密度估计算法。

    还要考虑数据的问题， 吃哦哦你个问了解数据。
    主要了解的是特征值，重点了解特征值的以下特征：
    + 离散型变量还是连续型变量
    + 特征值中是否有缺失的值， 何种原因导致的缺失
    + 数据中是否存在异常值
    + 某个特征发生的频率如何
*** 开发机器学习应用程序的步骤
    1. 收集数据
    2. 准备输入数据
    3. 分析输入数据
    4. 训练算法（无监督学习不需要训练算法）
    5. 测试算法
    6. 使用算法
**  KNN K近邻算法
    [[~/note/pictures/2.k-近邻算法.png]]
    - 优 点 :: 精度高、对异常值不敏感、无数据输入假定。
    - 缺 点 :: 计算复杂度高、空间复杂度高。
    - 适用数据范围 :: 数值型和标称型。
*** 一般流程
    1). 收集数据:可以使用任何方法。
    2). 准备数据:距离计算所需要的数值,最好是结构化的数据格式。
    3). 分析数据:可以使用任何方法。
    4). 训练算法:此步驟不适用于 1 近邻算法。
    5). 测试算法:计算错误率。
    6). 使 用 算法 :首先需要输入样本数据和结构化的输出结果,然后运行女-近邻算法判定输
    入数据分别属于哪个分类,最后应用对计算出的分类执行后续的处理。
**  决策树
*** 决策树的一般流程
    1. 收集数据： 可用任何方法
    2. 准备数据： 树构造算法只适用于标称型数据， 因此数值型数据必须离散化。
    3. 分析数据： 可使用任何方法，构造树完成后，应检查图形是否符合预期。
    4. 训练算法： 构造树的数据结构
    5. 测试算法： 适用经验树计算错误率
    6. 使用算法： 可适用于任何监督学习算法， 使用决策树可以更好理解数据的内在含义。
*** Questions
    1. 如果训练集中存在多个特征，第一次选择哪个特征作为划分的参考属性呢？
** 基于概率轮的分类方法： 朴素贝叶斯
   要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。

   + 优点 :: 在数据较少的情况下仍然有效，可以处理多类别问题。
   + 缺点 :: 对于输入数据的准备方式较为敏感。
   + 使用数据类型 :: 标称型数据。

   贝叶斯决策理论的核心思想： 选择高概率对应的类别。
*** 朴素贝叶斯的一般过程
    1. 收集数据： 可以用任何方法
    2. 准备数据： 需要数值型或者布尔型数据
    3. 分析数据： 有大量特征时， 绘制特征作用不大，此时使用直方图效果更好。
    4. 训练算法： 计算不同的独立特征的条件概率。
    5. 测试算法： 计算错误率。
    6. 使用算法： 常见的应用是文档分类。 可以在任何分类场景中使用朴素贝叶斯分类器， 不一定非要是文本。
*** 朴素贝叶斯的假设
    - 特征之间相互独立
    - 每个特征同等重要
** Logistic回归
   最优化算法， 基本的梯度， 及改进的随机梯度上升法
*** Logisic 回归的一般过程
    1. 收集数据： 任意方式
    2. 准备数据： 需要进行 距离计算， 要求数据类型是数值型， 结构化数据格式最佳。
    3. 分析数据： 任意方法对数据进行分析
    4. 训练算法： 大部分时间用于训练， 目的是找到最佳的分类回归系数
    5. 测试算法： 一旦训练步骤完成， 分类将会很快。
    6. 使用算法：
       - 需要输入数据， 并将其转换为结构化数值
       - 基于训练好的回归系数， 对数值进行简单的回归计算， 判定他们属于哪个类别
       - 此后， 可以在输出的类别上做一些其他的分析工作。


    + 优点 :: 计算代价不高， 易于理解和实现。
    + 缺点 :: 容易欠拟合， 分类精度可能不高。
    + 适用数据类型 :: 数值型和标称型数据。
*** 基于最优化方法的最佳回归系数确定
**** 梯度上升法
     梯度上升法的思想： 要找到某函数的最大值， 最好的方法是沿着该函数的梯度方向探寻。
***** 伪代码
      1. 每个回归系数初始化为1
      2. 重复R次：
         计算整个数据集的梯度
         使用alpha*gradient 更新回归系数的向量
      3. 返回回归系数
**** 随机梯度上升
     一次仅用一个样本点来更新回归系数。 可以进行增量式更新， 因此是一个在线学习算法。
***** 伪代码
      1. 所有回归系数初始化为1
      2. 对数据集中的每个样本
         i计算该样本的梯度
         使用alpha*gradient 更新回归系数值
      3. 返回回归系数
** 支持向量机
   支持向量（support vector）就是离分割超平面最近的哪些点。

   SVM的一般流程：
   1. 收集数据： 可以使用任意方法。
   2. 准备数据： 需要数值型数据。
   3. 分析数据： 有助于可视化分割超平面。
   4. 训练算法： SVM的大部分时间都源自训练，主要实现两个参数的调优。
   5. 测试算法： 简单的计算过程就可以实现。
   6. 使用算法： 几乎所有的分类问题都可以使用， svm本身是一个二类分类器， 如果对多类问题应用svm需要对代码做一些修改。
*** 简化版SMO
    伪代码如下：

    创建一个alpha向量并将其初始化为0向量
    当迭代次数小于最大迭代次数时（外循环）：
        对数据集中的每个数据向量（内循环）：
            如果该数据向量可以被优化：
               随机选择另一个数据向量
               同时优化这两个向量
               如果两个向量都不能被优化， 退出内循环
        如果所有向量都没有被优化，增加迭代数目， 继续下一次循环
*** 核函数(kernel)
    利用核函数(kernel)将数据映射到高维空间， 然后在高维空间中解决线性问题， 就等价于在低维空间中解决非线性问题。

    SVM优化中一个特别好的地方： 所有的运算都可以写成内积。 将内积替换称核函数的方式被称为核技巧。
** 利用AdaBoost元算法提高分类性能


* 利用回归预测数值型数据
** 预测数值型数据：回归
** 树回归

* 无监督学习
** 利用K-均值聚类算法对未标注数据分组
** 使用Apriori算法进行关联分析
** 使用FP-Growth 算法来高效发现频繁项集

* 其他工具
** 利用PCA来简化数据
** 利用SVD 简化数据
** 大数据与MapReduce
