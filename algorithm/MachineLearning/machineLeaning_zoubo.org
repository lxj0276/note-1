* 机器学习与数据分析
  主要内容:
    1. 机器学习与本课程示例概述
    2. 机器学习的角度看数学:
      - 数学分析
        导数与梯度
        Taylor 展式的应用
      - 概率论基础
        古典概型
        频率学派与贝叶斯学派
        常见概率分布
        Sigmoid/Logistic 函数的引入
* 概率论与贝叶斯先验
  主要内容:
    1. 概率论基础
      概率与直观
      常见概率分布
      Sigmoid/Logistic 函数的引入
    2. 统计量
      期望/方差/协方差/相关系数
      独立和不相关
    3. 大数定律
    4. 中心极限定理
    5. 最大似然估计
       估计一个概率模型参数的一种方法
      过拟合

  本福特定律(本福德法则, Frank Benford)，又称第一数字定律，是 指在实际生活得出的一组数据中， 以 1 为首位数字出现的概率约为总 数的三成;是直观想象 1/9 的三倍。
  - 满足本福特定律 ::  阶乘/素数数列/斐波那契数列首位  住宅地址号码  经济数据反欺诈  选举投票反欺诈

** 概率公式
   - 条件概率 :: P(A|B) = P(AB) / P(B)
   - 全概率公式 :: P(A) = ∑(i) P(A|B_i)P(B_i)
   - 贝叶斯(Bayes)公式 :: P(B_i|A) = P(A|B_i)P(B_i) / (∑(j) P(A|B_j)P(B_j))
                     P(A|B) = P(B|A)P(A) / P(B)
   给定样本 x, 计算系统的参数, 即: P(θ|x) = P(x|θ)P(θ) / P(x)
     - 先验概率 P(θ) :: 没有数据支持下, θ发生的概率
     - 后验概率 P(θ|x) :: 在数据 x 的支持下, θ 发生的概率
     - 似然函数 P(x|θ) :: 给定某参数θ 的概率分布
** 分布
   1. 0-1 分布
      | X | 1 |   0 |
      |---+---+-----|
      | p | p | 1-p |
      E(X) = P
      D(X) = pq
   2. 二项分布(Bernoulli distribution)
      1) 设随机变量 X 服从参数为 n, p 二项分布, 设 X_i 为第 i 次试验中事件 A 发生的次数, i=1, 2, ..., n, 则:
        X = ∑(i=1, n) X_i
         显然, X_i 相互独立均服从参数为 p 的 0-1 分布,
        所以: E(X) = np, D(X) = np(1-p)
      2) X 的分布律为 P{X=k} = (n, k) p^k (1-p)^(n-k) , (k=0, 1, 2..., n), 则:
         E(X) = ∑(k=0, n) kP{X=k} = np
         D(X) = E(X^2) = [E(X)]^2 = np(1-p) 
   3. 泊松分布(Poisson distribution)
      设 X ~ π(λ), 且分布律为:
        P{X = k} = (λ^k / k!) e^(-λ) ,  k=0, 1, 2, ..., λ>0.
      则有
        E(X) = λ, D(X) = λ

      在实际事例中，当一个随机事件，以固定的平均瞬时速率λ(或称密度)随机且独立地出现时，那么这个事件在单位时间(面积或体积) 内出现的次数或个数就近似地服从泊松分布 P(λ)。
      某一服务设施在一定时间内到达的人数.
   4. 均匀分布
      设 X ~ U(a, b), 其概率密度为
        f(x) = { 1/(b-a) , a < x < b ,
               { 0       , 其他 .

         则: E(X) = 1/2(a + b), D(X) = (b-a)^2 / 12
   5. 指数分布
      设随机变量 X 服从指数分布, 其概率密度为:
        f(x) = { (1/θ) e^(-x/θ) , x > 0, θ > 0
               { 0              , x > 0, θ > 0

        则: E(X) = θ, D(X) = θ^2
   6. 正态分布 X ~ N(μ, σ^2 )
      E(X) = μ, D(X) = σ^2
   7. Beta 分布(可以理解为概率的概率)
      Γ(n) = (n-1)!
      Γ(n) = (n-1)Γ(n-1)

      Beta 分布的期望: E(X) = α / α + β



| 分布     | 参数              | 数学期望 | 方差        |
|---------+-------------------+----------+------------- |
| 两点分布 | 0 < p < 1         | p        | p(1-p)      |
| 二项分布 | n >= 1, 0 < p < 1 | np       | np(1-p)     |
| 泊松分布 | λ > 0             | λ        | λ           |
| 均匀分布 | a < b             | (a+b)/2  | (b-a)^2 /12 |
| 指数分布 | θ > 0             | θ        | θ^2         |
| 正态分布 | μ, σ > 0          | μ        | σ^2         |
*** TODO 指数族

    - State "TODO"       from              [2017-04-15 Sat 17:51]
** 期望, 方差
** TODO 似然估计
   - State "TODO"       from              [2017-04-16 Sun 11:47]
* 矩阵和线性代数
矩阵的乘积意味着从 n 维空间到 m 维空间的变换
  1. 矩阵
    线性代数是有用的:以 SVD 为例
    矩阵的乘法/状态转移矩阵
    矩阵和向量组
  2. 特征值和特征向量
    对称阵、正交阵、正定阵
    数据白化
    正交基
    QR 分解/LFM
  3. 矩阵求导
    向量对向量求导
    标量对向量求导
    标量对矩阵求导
** SVD 奇异值分解(Singular Value Decomposition)
   svd 是一种重要的矩阵分解方法, 可以看做对称方阵在任意矩阵上的推广

   假设 A 是一个 m*n 阶实矩阵, 则存在一个分解使得:
     A_(m*n) = U_(m*m) Σ_(m*n) (V_(n*n))^T
   通常将奇异值由大而小排列. 这样, Σ便能由 A 唯一确定了

   与特征值, 特征向量的概念相对应:
     1. Σ 对角线上的元素成为 *矩阵 A 的奇异值*
     2. U 的第 i 列称为 *A 的关于σ_i 的左奇异向量*
     3. V 的第 i 列称为 *A 的关于σ_i 的右奇异向量*
** 方阵
*** 方阵的行列式
    定义:
    1 阶方阵的行列式为该元素本身; n 阶方阵的行列式等于它的任一行(/列)的各元素与其对应的代数余子式乘积之和.

    1 阶行列式等于该元素本身.
    n*n 的方阵, 其行列式用主对角线元素乘积减去次对角线元素的乘积.
*** 代数余子式
    在一个 n 阶行列式 A 中, 把(i, j)元素 a_ij 所在的第 i 行和第 j 列划去后, 留下的 n-1 阶方阵的行列式叫做元素 a_ij 的余子式, 记作 M_ij .
    代数余子式: A_ij = (-1)^(i+j) M_ij
*** 伴随矩阵
    对于 n*n 方阵的任意元素 a_ij 都有各自的代数余子式:
      A_ij = (-1)^(i+j) M_ij
    构造 n*n 的方阵 A^*
    则 A^* 称为 A 的伴随矩阵
    注意: A_ij 位于 A^* 的第 j 行第 i 列
*** 方阵的逆
    A A^* = |A| I

    A^-1 = A^* / |A|
*** 范德蒙行列式(Vandermonde)
*** 矩阵和向量的乘法
    1. A 为 m*n 的矩阵, x 为 n*1 的列向量, 则 Ax 为 m*1 的列向量, y = A x
    2. 由于 n 维列向量和 n 维空间的点一一对应, 上式实际给出了从 n 维空间的点到 m 维空间点的线性变换(在齐次坐标下的旋转, 平移)
    3. 如果 m=n, Ax 完成了 n 维空间内的线性变换
*** 平稳分布
    初始概率不同, 经若干次迭代, 最终稳定收敛在某个分布上.

    如果一个非周期的马尔科夫随机过程具有转移概率矩阵 P, 且它的任意两个状态都是连通的, 则极限存在 lim_(n->∞) (P_ij)^n = π(j)
*** 矩阵和向量的乘法
    - A 为 m*n 的矩阵, x 为 n*1 的列向量, 则 Ax 为 m*1 的列向量, 记为 Y=AX
    - 由于 n 维列向量和 n 维空间的点一一对应, 上式实际给出了从 n 维空间的点到 m 维空间点的线性变换. 旋转,平移(齐次坐标下)
    - 特殊的, 若 m=n, 且 Ax 完成了 n 维空间内的线性变换.
*** 矩阵的秩
    - k 阶子式 :: 在 m*n 矩阵 A 中, 任取 k 行 k 列, 不改变这 k^2 个元素在 A 中的次序, 得到 k 阶方阵.  m*n 矩阵 A 的 k 阶子式有(C_m)^k (C_n)^k 个
    - 矩阵 A 的秩(r) :: 设在矩阵 A 中有一个不等于 0 的 r 阶子式 D, 且所有 r+1 阶子式(如果存在的话)全等于 0, 那么, D 称为矩阵 A 的最高阶非零子式, r 成为矩阵 A 的秩, 记做 R(A)=r.

    1. n*n 的可逆矩阵, 秩为 n
    2. 可逆矩阵又称为 满秩矩阵
    3. 矩阵的秩等于它的行(列)向量组的秩

**** 秩与线性方程组解的关系
     对于 n 元线性方程组 Ax=b,
       1. 无解的充要条件是 R(A) < R(A, b)
       2. 有唯一解的充要条件是 R(A)=R(A,b)=n
       3. 有无限多解的充要条件是 R(A)=R(A,b)<n

     Ax=0 有非零解的充要条件是 R(A) < n
*** 向量组等价
    向量 b 能由向量组 A: a_1 , a_2 , ..., a_m 线性表示的充要条件是 矩阵 A 的秩等于矩阵 B=(a_1 , a_2 , ..., a_m , b)的秩

    设两个向量组 A, B, 若 B 组的向量都能由向量组 A 线性表示, 则称向量组 B 能由向量组 A 线性表示.
    若向量组 A 与向量组 B 能相互线性表示, 则称两个向量组等价.
*** 系数矩阵
    将向量组 A 和 B 所构成的矩阵依次记做 A, B, 若 B 能由 A 组线性表示, 则可以得到系数矩阵 K

    对于 C=A*B, 则矩阵 C 的列向量能由 A 的列向量线性表示, B 即这一表示的系数矩阵.

    向量组 B 能由向量组 A 线性表示的充要条件是矩阵 A 的秩等于矩阵(A,B)的秩, 即: R(A) = R(A, B)
*** 正交阵
    若 n 阶矩阵 A 满足 A^T A = I, 称 A 为正交矩阵, 简称正交阵.

    A 是正交阵的充要条件: A 的列(行)向量都是 *单位向量*, 且两两正交.

    A 是正交阵, x 为向量, 则 Ax 称作正交变换. 正交变换不改变向量长度.
*** 特征值, 特征向量
** 正定阵
   判定: 以下三条定价
   - 对称阵 A 为正定阵
   - A 的特征值都为正
   - A 的顺序主子式大于 0
* python 基础
* python 库
  1. 信息摘要与安全哈希算法 MD5/SHA1
  2. 统计量: 均值, 方差, 偏度, 峰度
  3. 多元告高斯分布
  4. 阶乘的实数域推广: Γ函数
  5. Pearson 相关系数的计算
  6. 快速傅里叶变量 FFT 与信号滤波
  7. 奇异值分解 SVD 与图像特征
  8. 股票数据相关: 收盘价格曲线, 滑动平均线, K 线图
  9. 图像的卷积与卷积网络 CNN
  10. 蝴蝶效应: Lorenz 系统的曲线生产
** 统计量
*** 偏度
    偏度衡量随机变量概率分布的不对称性, 是相对于均值不对称程度的度量.
* 数据清洗和特征选择
  内容
     庄家与赔率
     Nagel-Schreckenberg 交通流模型
     Pandas 数据读取和处理
     Fuzzywuzzy(一个模块) 字符串模糊查找,替换(数据清洗)
     数据清洗和校正
     特征提取主成分分析 PCA
     One-hot 编码
   思考:
     字符串编辑距离
     RO C 曲线 与 AUC(Area Under Curve)
     分类器:随机森林、Logistic 回归

    准确率(accuracy), 混淆矩阵, TPR, FPR
    Precision
    Recall
    F1-measure
    AUC
    AIC/BIC
* 回归
lasso 可以用来降维
   线性回归
     高斯分布
     最大似然估计 MLE
     最小二乘法的本质
   Logistic 回归
     分类问题的首选算法
   多分类:Softmax 回归
     目标函数
   技术点
     梯度下降算法
     最大似然估计
     特征选择
** 假设
    假设具有三个性质:
     内涵性
     简化性
     发散性
*** 假设的内涵性
   所谓假设，就是根据常理应该是正确的。
    如假定一个人的身高位于区间[150cm,220cm]， 这能够使得大多数情况都是对的，但很显然有 些篮球运动员已经不属于这个区间。所以，假 设的第一个性质:假设往往是正确的但不一定 总是正确。
   我们可以称之为“假设的内涵性”。
*** 假设的简化性
   假设只是接近真实，往往需要做若干简化。
     如，在自然语言处理中，往往使用词袋模型 (Bag Of Words)，认为一篇文档的词是独立的— —这样的好处是计算该文档的似然概率非常简 洁，只需要每个词出现概率乘积即可。
     但我们知道这个假设是错的:一个文档前一个 词是“正态”，则下一个词极有可能是“分 布”，文档的词并非真的独立。
     这个现象可以称之为“假设的简化性”。
* 决策树和随机森林

graphviz

样本不均衡的常用处理方法:
  1. 降采样(在决策树的随机森林比重采样好一点)
  2. 重采样
  3. 数据合成, 随机插值
  4. 增降权值

最大似然估计(最大) = Kl(散度最小) = 负对数似然(最小) = 交叉熵(最小)

信息熵 venn 图

决策树避免发生过拟合现象: 1. 剪枝  2. 随机森林 

  1. gini 系数的理论意义是什么
  2. 决策树剪枝到只有 1 个结点是什么意思
* 提升及 xgboost 实践
** Adaboost 和 GBDT 的区别
   Adboost 是 GBDT 的一个特例或者 GBDT 是对 Adboost 进行推广.
   最主要的区别在于两者如何识别模型的问题。AdaBoost 用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。Gradient Boosting 通过负梯度来识别问题，通过计算负梯度来改进模型。
   gbdt 更新的是样本标签 ，adaboost 更新的是样本的权重.

   Adboost 是在每一轮提升相应错分类点的权重可以被理解为调整错分类点的 Observation probability.
   和 Adaboost 一样, Gradient Boosting 也是重复选择一个表现一般的模型, 并且每次基于先前模型的表现进行调整. 不同的是, Adaboost 是通过提升错分数据点的权重来定位模型的不足, 而 Gradient Boosting 可以使用更多种类的目标函数(通过算梯度来定位模型的不足).

   拟牛顿法
* svm 及其实践
* 聚类 及其实践
* EM 算法及其实践
* 贝叶斯网络及其实践
* 主题模型及其实践
* HMM 及其实践
* FQA
  1. Hessian 矩阵
  2. 正定, 半正定矩阵
  3. 动态规划
  4. LASSO
  5. 从概率论的角度:
     - Least Square 的解析解可以用 Gaussian 分布及最大似然估计求得
     - Ridge 回归可以用 Gaussian 分布和最大后验估计解释
     - Lasso 回归可以用 Laplace 分布和最大后验估计解释
  6. 
